{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tajfsk_7JY3E"
   },
   "source": [
    "**Note to grader:** Each question consists of parts, e.g. Q1(i), Q1(ii), etc. Each part must be graded  on a 0-4 scale, following the standard NJIT convention (A:4, B+: 3.5, B:3, C+: 2.5, C: 2, D:1, F:0). \n",
    "The total score must be re-scaled to 100 -- that should apply to all future assignments so that Canvas assigns the same weight on all assignments. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPnHTf9MfT5X"
   },
   "outputs": [],
   "source": [
    "# Grader's area\n",
    "import numpy as np\n",
    "M = np.zeros([10,10])\n",
    "maxScore = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SArgW_Vq-uTh"
   },
   "source": [
    "# **Assignment 3**\n",
    "\n",
    "The goal in this assignment to work a little more with Python, do some practice with logistic regression, reflect on how it can fail to work on linearly separable data. You will also work with a support vector classifier. All that, still on \"toy\" data sets. \n",
    "\n",
    "We will work with the first 'real' data sets in the next assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlFM4hig-uTj"
   },
   "source": [
    "## **Preparation Steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3alYkjM-uTk"
   },
   "outputs": [],
   "source": [
    "# Import all necessary python packages\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eTVa-fum-uTt",
    "outputId": "ac672805-93aa-4f90-8052-ea57a66861c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n"
     ]
    }
   ],
   "source": [
    "# ### Reading-in the Iris data\n",
    "\n",
    "s = os.path.join('https://archive.ics.uci.edu', 'ml',\n",
    "                 'machine-learning-databases', 'iris','iris.data')\n",
    "s = s.replace(\"\\\\\",\"/\");\n",
    "print('URL:', s)\n",
    "df = pd.read_csv(s,header=None,encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "cqzyfFCC-uTz",
    "outputId": "1d89f90c-99a4-4755-9418-27bba70817a1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select setosa and versicolor for binary classification\n",
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-setosa', -1, 1)\n",
    "\n",
    "# extract sepal length and petal length\n",
    "X = df.iloc[:100, [0, 2]].values\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:50, 0], X[:50, 1],\n",
    "            color='red', marker='o', label='setosa')\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1],\n",
    "            color='blue', marker='x', label='versicolor')\n",
    "\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "\n",
    "# plt.savefig('images/02_06.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYEgTzp0s5BN"
   },
   "outputs": [],
   "source": [
    "# function for visualizing decision regions\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class examples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=cl, \n",
    "                    edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phGY3AEGn3dz"
   },
   "source": [
    "-----------------------\n",
    "-----------------------\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe9kKR3J-uUA"
   },
   "source": [
    "## <font color = 'blue'> **Question 1. Practice with logistic regression** </font>\n",
    "\n",
    "Let's first load the textbook's implementation of logistic regression with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSRTXCwz-uUA"
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionGD(object):\n",
    "    \"\"\"Logistic Regression Classifier using gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting.\n",
    "    loss_ : list\n",
    "      Logistic loss function value in each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.05, n_iter=100, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_examples, n_features]\n",
    "          Training vectors, where n_examples is the number of examples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_examples]\n",
    "          Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "        self.loss_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors = (y - output)\n",
    "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
    "            self.w_[0] += self.eta * errors.sum()\n",
    "            \n",
    "            # compute the logistic `loss` \n",
    "            loss = -y.dot(np.log(output)) - ((1 - y).dot(np.log(1 - output)))\n",
    "            self.loss_.append(loss)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def activation(self, z):\n",
    "        \"\"\"Compute logistic sigmoid activation\"\"\"\n",
    "        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, 0)\n",
    "        # equivalent to:\n",
    "        # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMYBgs4E-uUF"
   },
   "source": [
    "Below you can see the first 3 data points of the data set, all labeled as 'setosa'. Let's set the numerical value for 'setosa' to 1. (i.e. y = 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyoLJVet-uUG",
    "outputId": "9ff50e48-0453-4fef-a704-5a7f81355a6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4],\n",
       "       [4.9, 1.4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPbGfqmf-uUK"
   },
   "source": [
    "\n",
    "Suppose the initial weights of the logistic neuron are w0=0.1, w1=-0.2, w2=0.1\n",
    "\n",
    "<font color = 'blue'> **Q1-1**.  </font> Write the weights after processing data points 0,1,2, with learning rate $\\eta=0.1$ and show your calculations. This is similar to the previous assignment, only done now for the logistic neuron. You can also use *LogisticRegressionGD* to check your calculations. <br>\n",
    "\n",
    "<font color = 'blue'> **Q1-2**.  </font> Given our data $X$, let $X_{d=2}$ and $X_{d=3}$ be the quadratic and cubic features. Using code from the notebook on polynomial regression, generate $X_{d=2}$ and $X_{d=3}$\n",
    "\n",
    "<font color = 'blue'> **Q1-3**.  </font> Using *LogisticRegressionGD* fit $X$, $X_{d=2}$ and $X_{d=3}$. Here you should set $\\eta = 0.1$ and $n_{iter}>1000$. For each of these three cases, report the loss function value for the model computed by *LogisticRegressionGD*.\n",
    "Here it is expected that the loss value decreases as $d$ increases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wYFal09ID7Q"
   },
   "outputs": [],
   "source": [
    "# your calculations and code go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HV6DRoDxG1Rl"
   },
   "outputs": [],
   "source": [
    "# Grader's area\n",
    "\n",
    "maxScore = maxScore +12\n",
    "#M[1,1] = \n",
    "#M[1,2] = \n",
    "#M[1,3] ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6grYJ9spn6C1"
   },
   "source": [
    "-----------------------\n",
    "-----------------------\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zW8T7xf0-uUL"
   },
   "source": [
    "## <font color = 'blue'> **Question 2. A theoretical question** </font>\n",
    "\n",
    "This question is about a theoretical explanation for what you observed in question 1(iii). \n",
    "\n",
    "<br>\n",
    "\n",
    "Suppose $f_1$ is a model that optimally fits the data $(X,y)$, and $f_2$ is another model that optimally fits the data $(X_2,y)$ where $X_2$ are the quadratic features of $X$. Then the loss function value obtained by $f_2$ is **always** going to be at least equal to that for $f_1$. Try to come up with a solid mathematical argument that justifies this claim. [Note: as with anything else, feel free to discuss this on Canvas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRk7QbZLH_DY"
   },
   "source": [
    "(your answer goes here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJrBDgJYBQZL"
   },
   "outputs": [],
   "source": [
    "# Grader's area\n",
    "\n",
    "maxScore = maxScore + 4\n",
    "#M[2,1] = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxiK-qoin6_-"
   },
   "source": [
    "-----------------------\n",
    "-----------------------\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMEcdAp3-uT-"
   },
   "source": [
    "##  <font color = 'blue'> **Question 3. Logistic Regression: How 'unfair' can it be?**  </font>\n",
    "\n",
    "We have seen that the inductive bias of an SVC guarantees that when the dataset is linearly separable, SVC will return a hyperplane that is at exactly the same distance from the two classes.  But what about logistic regression? Can we guarantee that it can also be at least partially fair?\n",
    "\n",
    "The answer is **negative**. We can  demonstrate how logistic regression can be 'unfair' by constructing a dataset with the properties that: <br> \n",
    "\n",
    "**a.** the data set is linearly separable  <br>\n",
    "**b.** the optimal logistic regression model corresponds to a hyperplane that nearly 'touches' one of the two classes, i.e. it gives a very big margin to the one of the two classes and very small margin for the other class.  <br>\n",
    "\n",
    "Demonstrate your answer as follows: <br>\n",
    "\n",
    "<font color = 'blue'> **Q3-1**.  </font>\n",
    " Plot the data points, as we did above for the iris data set. This will show that your data set is linearly separable. <br>\n",
    "<font color = 'blue'> **Q3-2**.  </font> Calculate the optimal logistic neuron weights using the function *LogisticRegressionGD* from question 1. <br>\n",
    "<font color = 'blue'> **Q3-3**. </font>  Plot the decision regions to demonstrate how the learned separation line is unfair. \n",
    "\n",
    "**Hint**: Try small datasets.\n",
    "**Note**: It's best to use fresh variables for your dataset, because the previous $X,y$ are re-used in question 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LejWjUeapllz"
   },
   "outputs": [],
   "source": [
    "# your answers go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTPbCgxvkoA9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayVRy0pxlBDP"
   },
   "outputs": [],
   "source": [
    "# Grader's area\n",
    "\n",
    "maxScore = maxScore + 12\n",
    "#M[3,1] = \n",
    "#M[3,2] = \n",
    "#M[3,3] = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2ZazaTq1GzQ"
   },
   "source": [
    "<font color = 'blue'> **Q3-4**.  </font> The standard scikit-learn implementation of logistic regression uses regularization by default ($C=1$).  Can you come up with a linearly separable dataset that makes that **default** implementation fail? <br>\n",
    "\n",
    "[Note: This is an experimental question. You should be able to use the example from above, or modify it, and make the default implementation fail.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrSMBWyD2mKC"
   },
   "outputs": [],
   "source": [
    "# your answer goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XN-VON-12oqM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlN9ptsC2rz_"
   },
   "outputs": [],
   "source": [
    "# Grader's area\n",
    "maxScore = maxScore + 4\n",
    "\n",
    "# M[3,4] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kffsOAaln8C_"
   },
   "source": [
    "-----------------------\n",
    "-----------------------\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVUZKNB2kod6"
   },
   "source": [
    "##  <font color = 'blue'> **Question 4. SVC and classification margin**  </font>\n",
    "\n",
    "The Iris dataset defined in above cells is linearly separable. \n",
    "\n",
    "<font color = 'blue'> **Q4-1.**  </font> Use a [linear SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) to learn a hyperplane $y=w_1x_1 +w_2x_2 +b$ that maximizes the margin for this Iris dataset. In your answer, specify a setting for the hyperparameter $C$ that reduces the amount of regularization, i.e. it incentivizes very small slacks.\n",
    "\n",
    "<font color = 'blue'> **Q4-2.** </font> Extract the coefficients $w$ and the intercept $b$ from the learned SVC. Find the the 2-norm of $w$, $s = ||w||_2$.\n",
    "[Hint: Read the documentation in order to access the coefficients] \n",
    "\n",
    "<font color = 'blue'> **Q4-3.** </font> Set $w = w/s$ and $b=b/s$. This changes the numerical definition of the separation line, but the line is still the same.  \n",
    "\n",
    "<font color = 'blue'> **Q4-4.** </font> With the new $w$ and $b$, calculate $wx^T - b$ for each point $x$ in our dataset. This will give a range of values, let $\\gamma$ be the smallest one in absolute value. This $\\gamma$ is the margin (in fact there should be two points $x_1$ and $x_2$ of different labels, that give $wx_1^T - b = \\gamma$ and $wx_1^T - b = -\\gamma$). \n",
    "\n",
    "\n",
    "**Note:** as with anything else, feel free to discuss this on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64Pwo6Mxmv6X"
   },
   "outputs": [],
   "source": [
    "# Grader's area\n",
    "\n",
    "maxScore = maxScore + 16\n",
    "#M[4,1] = \n",
    "#M[4,2] = \n",
    "#M[4,3] = \n",
    "#M[4,4] = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTLcwDVan9x9"
   },
   "source": [
    "-----------------------\n",
    "-----------------------\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3ECKgmBD_aJ"
   },
   "source": [
    "##  <font color = 'blue'> **Question 5. Upper bound for perceptron errors before convergence**  </font>\n",
    "\n",
    "In this problem we will use $\\gamma$ calculated in question 4. If you have not been able to calculate it, you can 'borrow' its value from someone else that has calculated it (I expect that this can be done on Canvas).\n",
    "\n",
    "<font color = 'blue'> **Q5-1.** </font> Calculate $R = \\max_{x\\in X} ||x||_2$. In other words, find the 2-norm of all points in the dataset, and let $R$ be the maximum norm. \n",
    "\n",
    "<font color = 'blue'> **Q5-2.** </font> Calculate $maxErrors = R/\\gamma^2$. This is the maximum number of errors that a perceptron can do, in the worst case, before it convergences. \n",
    "\n",
    "<font color = 'blue'> **Q5-3.** </font> Compare $maxError$ with the actual number of errors that the perceptron does with a random initialization. [Hint: Re-use the code from assignment 2, or anything else that can provide you with that number].  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccvq5Z_RI8nO"
   },
   "outputs": [],
   "source": [
    "# your answers go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KT8yPatZI-US"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUts9oNyI4R7"
   },
   "outputs": [],
   "source": [
    "# Grader's area\n",
    "\n",
    "maxScore = maxScore + 12\n",
    "#M[5,1] = \n",
    "#M[5,2] = \n",
    "#M[5,3] = \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdAULOJaI_2h"
   },
   "source": [
    "----------------------------\n",
    "----------------------------\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVkneTzYCAxs"
   },
   "outputs": [],
   "source": [
    "#Grader's area\n",
    "\n",
    "rawScore = np.sum(M)\n",
    "score = rawScore*100/maxScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHdcyebzleMQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
